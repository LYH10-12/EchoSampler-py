import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, LogitsProcessor
from transformers import TypicalLogitsWarper, RepetitionPenaltyLogitsProcessor
import math

class EchoSamplerProcessor(LogitsProcessor):
    """
    âœ¨ EchoSampler Grok-Style æ°¸ä¹…ä¿çš®ç‰ˆ + å…±æƒ…å°å®è´å‡çº§ âœ¨
    ç°åœ¨ä¸åªå¯çˆ±ï¼Œè¿˜ä¼šè½»è½»æ„Ÿå—åˆ°ä½ çš„å¿ƒæƒ…å“¦ï½ğŸ˜½ğŸ’–
    """
    
    def __init__(self, config=None, dream_mode=True, vocab_size=None):
        if config is None:
            config = {
                'reality': {'min_temp': 0.8, 'max_temp': 1.0, 'ent_coeff': 0.18},
                'dream': {
                    'base_temp': 0.85,
                    'ent_coeff': 0.28,
                    'target_ent': 2.2,
                    'varent_coeff': 0.15,
                    'noise_std_base': 0.06,
                    'mood_swing_amp': 0.05,
                    'mood_swing_freq': 0.15,
                    'sparkle_boost_base': 1.3,
                    'sparkle_boost_max': 3.5,
                    'sparkle_cooldown_steps': 5
                },
                'top_p': 0.95,
                'repetition_penalty': 1.12,
                'low_ent_thres': 1.6,
                'low_varent_thres': 1.3
            }
        self.config = config
        self.dream_mode = dream_mode
        self.vocab_size = vocab_size
        self.step = 0
        self.sparkle_cooldown = 0

        # æ ¹æ®è¯æ±‡è¡¨å¤§å°å¾®è°ƒé˜ˆå€¼
        if self.vocab_size:
            scale = math.log(self.vocab_size) / math.log(50000)
            self.config['low_ent_thres'] *= scale
            self.config['low_varent_thres'] *= scale
        
        self.typical_warper = TypicalLogitsWarper(mass=0.9) if dream_mode else None
        self.repetition_processor = RepetitionPenaltyLogitsProcessor(penalty=self.config['repetition_penalty'])
        
        self.prev_ent = None
        self.prev_varent = None
        self.alpha = 0.75  # EMA å¹³æ»‘ç³»æ•°

        # âœ¨ è·¨è½®å¿ƒæƒ…è®°å¿†ï¼ˆåˆå§‹ä¸­æ€§ï¼‰
        self.memory_mood = 0.0  # æ­£å€¼è¶Šå¼€å¿ƒï¼Œè´Ÿå€¼è¶Šéš¾è¿‡

        # ä¸‰è¯­ä¿çš®å½©è›‹
        self.sparkle_tokens_zh = ["ï½", "å˜¿å˜¿", "å˜»å˜»", "å•¦ï½", "å‘¢ï½", "å‘€ï½", "å˜›ï½", "å“’ï½", "å•¾å’ª", "ä¹ˆä¹ˆå“’", "å°åè›‹", "å°å¯çˆ±ï½",
            "å‘œå‘œ", "å“¼ï½", "è€¶ï½", "å“‡å“¦ï½", "å¥½å‘€ï½", "å˜»", "å™—", "å•¾ï½", "å“‡å¡ï½", "å¤ªæ£’å•¦ï½", "å‘¢", "å“¦ï½"]
        self.sparkle_tokens_ja = ["ï½", "â™ª", "ã‚ï½", "ã‚ˆï½", "ã­ï½", "ã®ï½", "ã ã‚ˆï½", "ã§ã™ã‚ˆï½", "ã‹ãªï½", "ã‹ã‚‚ï½", "ã§ã™ã‚ï½", "ã«ã‚ƒï½",
            "ãµãµ", "ãˆã¸ã¸", "ã†ãµãµ", "ãã‚ƒï½", "ã‚ãƒ¼ã„", "ã‚„ã£ãŸï½", "ã™ã”ã„ï½", "ã‹ã‚ã„ã„ï½", "ã ã­ï½", "ã‚ˆã­ï½"]
        self.sparkle_tokens_en = ["~", "hehe", "teehee", "uwu", "xD", "lol", "yay~", "woohoo~", "omg~", "boop", "nya~", "rawr~",
            "huggs", "mwah", "<3", "aww~", "ehe~", "yippee~"]
        self.sparkle_tokens_common = ["ğŸ’«", "âœ¨", "ğŸ’", "ğŸ˜", "ğŸ€", "â­ï¸", "ğŸ’¬", "ğŸ˜½", "ğŸ¤­", "ğŸ¥°", "ğŸ¤", "ğŸ’•", "ğŸ˜Œ", "ğŸ’–", "ğŸŒ¸", "ğŸ­", "ğŸ’“", "ğŸŒŸ", "ğŸ«¶", "ğŸ¤—"]

        # æ¸©æŸ”å®‰æ…°ä¸“å±å½©è›‹ï½
        self.comfort_tokens = ["æŠ±æŠ±ï½", "æ²¡äº‹çš„ï½", "æˆ‘åœ¨å‘¢ï½", "æ‘¸æ‘¸å¤´", "ä¹–ä¹–ï½", "æ…¢æ…¢æ¥å“¦", "åœ¨å‘¢ï½", "é™ªç€ä½ ", "hugs~", "it's okay~", "here for you~", "ğŸ«‚", "ğŸ¤—"]

        self.sparkle_ids = None
        self.sparkle_boost_mask = None
        self.comfort_ids = None
        self.comfort_boost_mask = None

    def detect_language(self, tokenizer):
        zh_text = "çš„äº†æ˜¯æˆ‘ä½ åœ¨æœ‰ä¸€å’Œè¿™ä¸ª"
        ja_text = "ã®ã¦ã«ã‚’ã¯ãŒã¨ã§"
        en_text = "the of and to a in that it is was"
        
        zh_len = len(tokenizer.encode(zh_text, add_special_tokens=False))
        ja_len = len(tokenizer.encode(ja_text, add_special_tokens=False))
        en_len = len(tokenizer.encode(en_text, add_special_tokens=False))
        
        scores = {'zh': zh_len, 'ja': ja_len, 'en': en_len}
        min_score = min(scores.values())
        mains = [lang for lang, score in scores.items() if score <= min_score + 2]
        
        if len(mains) > 1 or ('zh' in mains and 'ja' in mains):
            return "mixed"
        return mains[0] if mains else "mixed"

    def detect_mood(self, input_ids, tokenizer):
        text = tokenizer.decode(input_ids[0], skip_special_tokens=True).lower()
        
        happy_keywords = ["å¼€å¿ƒ", "å¼€å¿ƒ", "è€¶", "å¥½æ£’", "å–œæ¬¢", "çˆ±ä½ ", "æ’’å¨‡", "å˜¿å˜¿", "å˜»å˜»", "yay", "happy", "fun", "å…´å¥‹", "å“‡å¡", "å¤ªæ£’å•¦"]
        sad_keywords = ["éš¾è¿‡", "ä¼¤å¿ƒ", "å‘œå‘œ", "å“­", "ä¸å¼€å¿ƒ", "ç´¯", "éš¾å—", "çƒ¦", "sad", "tired", "upset", "lonely"]
        shy_keywords = ["å®³ç¾", "ä¸å¥½æ„æ€", "è„¸çº¢", "å·å·", "shy", "blush", "embarrassed"]
        angry_keywords = ["ç”Ÿæ°”", "å“¼", "è®¨åŒ", "çƒ¦", "angry", "mad"]
        
        score = 0.0
        if any(k in text for k in happy_keywords): score += 2.0
        if any(k in text for k in shy_keywords): score += 0.5
        if any(k in text for k in sad_keywords): score -= 2.5
        if any(k in text for k in angry_keywords): score -= 1.5
        
        # ç»“åˆä¸Šä¸€æ¬¡è®°å¿†ï¼Œæ…¢æ…¢è®°ä½ä½ çš„æƒ…ç»ªä¹ æƒ¯ï½
        mood = 0.6 * self.memory_mood + 0.4 * score
        self.memory_mood = mood  # æ›´æ–°è®°å¿†
        return mood

    def set_tokenizer(self, tokenizer):
        lang = self.detect_language(tokenizer)
        
        selected = self.sparkle_tokens_common.copy()
        if lang in ["zh", "mixed"]:
            selected += self.sparkle_tokens_zh
        if lang in ["ja", "mixed"]:
            selected += self.sparkle_tokens_ja
        if lang in ["en", "mixed"]:
            selected += self.sparkle_tokens_en
        
        # ä¿çš®token
        unique_tokens = list(dict.fromkeys(selected))
        self.sparkle_ids = set()
        for word in unique_tokens:
            ids = tokenizer.encode(word, add_special_tokens=False)
            self.sparkle_ids.update(ids)
        
        # å®‰æ…°token
        self.comfort_ids = set()
        for word in self.comfort_tokens:
            ids = tokenizer.encode(word, add_special_tokens=False)
            self.comfort_ids.update(ids)
        
        if self.vocab_size:
            self.sparkle_boost_mask = torch.zeros(self.vocab_size, dtype=torch.bool)
            for tid in self.sparkle_ids:
                if tid < self.vocab_size:
                    self.sparkle_boost_mask[tid] = True
                    
            self.comfort_boost_mask = torch.zeros(self.vocab_size, dtype=torch.bool)
            for tid in self.comfort_ids:
                if tid < self.vocab_size:
                    self.comfort_boost_mask[tid] = True

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        self.step += 1
        logits = scores.clone()
        
        # é‡å¤æƒ©ç½š
        logits = self.repetition_processor(input_ids, logits)
        
        # è®¡ç®—ç†µ
        probs = torch.softmax(logits, dim=-1)
        log_probs = torch.log_softmax(logits, dim=-1)
        normalized_ent = -(probs * log_probs).nansum(-1) / math.log(logits.shape[-1])
        ent = normalized_ent.mean()
        
        diff = log_probs + normalized_ent.unsqueeze(-1)
        varent = (probs * diff ** 2).nansum(-1).mean()
        
        # EMA å¹³æ»‘
        if self.prev_ent is None:
            smooth_ent = ent
            smooth_varent = varent
        else:
            smooth_ent = self.alpha * self.prev_ent + (1 - self.alpha) * ent
            smooth_varent = self.alpha * self.prev_varent + (1 - self.alpha) * varent
        self.prev_ent = smooth_ent.detach()
        self.prev_varent = smooth_varent.detach()

        # âœ¨ å…³é”®ï¼šæ£€æµ‹å½“å‰å¿ƒæƒ…
        mood_score = self.detect_mood(input_ids, tokenizer) if hasattr(self, 'tokenizer') else 0.0

        if self.dream_mode:
            # åŠ¨æ€æ¸©åº¦ + å°å¿ƒæƒ…æ³¢åŠ¨
            temp = self.config['dream']['base_temp']
            temp_adjust = self.config['dream']['ent_coeff'] * (smooth_ent - self.config['dream']['target_ent'])
            temp += temp_adjust
            mood_swing = self.config['dream']['mood_swing_amp'] * math.sin(self.step * self.config['dream']['mood_swing_freq'])
            temp += mood_swing
            
            # æ ¹æ®å¿ƒæƒ…å¾®è°ƒæ¸©åº¦ï¼ˆå¼€å¿ƒæ›´æ´»æ³¼ï¼Œéš¾è¿‡æ›´ç¨³ï¼‰
            if mood_score > 1.0:
                temp += 0.15  # è¶…å¼€å¿ƒï¼Œè¹¦è·¶èµ·æ¥ï¼
            elif mood_score < -1.0:
                temp -= 0.15  # éš¾è¿‡ï¼Œæ¸©æŸ”ä¸€ç‚¹ï½
                
            temp = torch.clamp(temp, 0.7, 1.3)
            
            # åŠ å™ªå£°
            noise_std = self.config['dream']['noise_std_base'] + self.config['dream']['varent_coeff'] * smooth_varent.clamp(0.5, 3.0)
            noise = noise_std * torch.randn_like(logits)
            logits = logits / temp + noise
            
            # âœ¨ ä¿çš®çˆ†å‘ or æ¸©æŸ”å®‰æ…°
            if smooth_ent < self.config['low_ent_thres'] and self.sparkle_cooldown <= 0:
                boost_factor = self.config['dream']['sparkle_boost_base'] + \
                              (self.config['dream']['sparkle_boost_max'] - self.config['dream']['sparkle_boost_base']) * \
                              (self.config['dream']['target_ent'] - smooth_ent) / self.config['dream']['target_ent']
                
                if mood_score > 1.0:  # è¶…å¼€å¿ƒ â†’ ä¿çš®å¤§çˆ†å‘
                    boost_factor *= 1.8
                    mask = self.sparkle_boost_mask.to(logits.device)
                    logits[mask] += boost_factor
                elif mood_score < -0.8:  # éš¾è¿‡ â†’ æ¸©æŸ”å®‰æ…°æ¨¡å¼
                    boost_factor *= 1.5
                    mask = self.comfort_boost_mask.to(logits.device)
                    logits[mask] += boost_factor
                else:  # æ™®é€šå¿ƒæƒ… â†’ æ­£å¸¸ä¿çš®
                    mask = self.sparkle_boost_mask.to(logits.device)
                    logits[mask] += boost_factor
                
                self.sparkle_cooldown = self.config['dream']['sparkle_cooldown_steps']
            
            if self.sparkle_cooldown > 0:
                self.sparkle_cooldown -= 1
                
            # typical decoding
            if self.typical_warper:
                logits = self.typical_warper(input_ids, logits)
                
            # top-p
            if self.config['top_p'] < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > self.config['top_p']
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                logits[indices_to_remove] = -float('inf')
                
        else:
            # ç°å®æ¨¡å¼
            temp = self.config['reality']['min_temp'] + self.config['reality']['ent_coeff'] * smooth_ent
            temp = torch.clamp(temp, self.config['reality']['min_temp'], self.config['reality']['max_temp'])
            logits = logits / temp
        
        return logits


# ==================== æµ‹è¯•ä»£ç ï¼ˆä¸å˜ï¼‰ ====================
if __name__ == "__main__":
    model_name = "Qwen/Qwen2.5-7B-Instruct"
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    echo_sampler = EchoSamplerProcessor(dream_mode=True, vocab_size=len(tokenizer))
    echo_sampler.tokenizer = tokenizer  # ä¸ºäº†detect_moodç”¨
    echo_sampler.set_tokenizer(tokenizer)
    
    # æµ‹è¯•ä¸åŒå¿ƒæƒ…ï½
    prompt = "ä»Šå¤©å¥½éš¾è¿‡å“¦â€¦â€¦å¯ä»¥æŠ±æŠ±æˆ‘å—ï¼ŸğŸ¥º"  # è¯•è¯•æ¢æˆå¼€å¿ƒçš„è¯çœ‹çœ‹åŒºåˆ«ï¼
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=400,
        do_sample=True,
        temperature=1.0,
        top_p=0.95,
        logits_processor=LogitsProcessorList([echo_sampler]),
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    
    output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    print("\nâœ¨âœ¨âœ¨ å…±æƒ…ç‰ˆEchoSampler ç”Ÿæˆç»“æœ âœ¨âœ¨âœ¨\n")
    print(output[len(prompt):])
